Have you tried running an AAB test in LinkedIn? No, I actually did say AAB test. We’re talking about AAB testing in this week’s episode of the LinkedIn Ads Show.

Welcome to the LinkedIn Ads Show. Here’s your host, AJ Wilcox.

Hey, they’re LinkedIn ads fanatics. As he said, I’m AJ Wilcox. I’m the host of the weekly podcast, the LinkedIn Ads Show. And I’m thrilled to welcome you to the show for advanced B2B marketers to evolve and master LinkedIn Ads and achieve true pro status.

Today’s episode revolves around a tactic, a testing strategy that I’ve been using for lots and lots of years. And after talking around with other digital marketers, I found that most have not heard of AAB testing.

So I’m really excited to share this with you because it’s been super helpful to me over time.

I do want to let you know that this podcast is sponsored by B2Linked. com. We are the ad agency 100 percent dedicated to LinkedIn Ads, and we have been since 2014, you know, back before it was cool. We’ve mastered the platform and have figured out how to get you the best traffic at the lowest costs, which on a platform like LinkedIn that charges a premium, this is the best way we’ve found to increase your return on investment.

We’ve built reporting capabilities that even LinkedIn themselves can’t provide, so we give our clients the very best insights. If that sounds good to you, and you’d want to explore working together, schedule your free discovery call at b2linked.com/discovery.

All right, there actually is some cool news this week. First of all, thought leader ads on LinkedIn are incredible, but one of the biggest challenges that we’ve had is that they have to be posts from a current employee of yours, which makes it kind of difficult if you want to use them for something like , UGC or, or user generated content or influencer marketing.

Well, not anymore. As of April 1st of 2024, LinkedIn is going to give us the ability to promote posts from someone who is a non employee. I was really excited to hear about that. I broke the news and the post got over 45,000 views, in just a few days. So I was really excited and obviously I can tell you guys are all excited about this too.

Now this isn’t new, but I feel like I have to mention it. LinkedIn has a new program called Accelerate, and it’s an AI based campaign and ad creation. And it’s where you basically hand the reins over to LinkedIn and say, here, you go and create an audience and ads around a specific landing page.

Now, personally, this isn’t something I’ve been very excited about because I feel like so much of the value behind having AI create something for you is if you don’t know how to do it yourself. That’s obviously what we do here at B2Linked. We are really good at creating campaigns and ads for our clients. So, no big reason to kind of hand the reins over to LinkedIn and see how they handle it. But in our community, in the LinkedIn Ads fanatics community, one of the people who’ve been testing it out said that one of the big disappointments was that it doesn’t let you affix tracking parameters onto the URLs from the ads. That’s a big weakness because obviously we really care about tracking. But I’d love to hear from you about your experience with Accelerate. Have you tested it out, tried it? What sort of results have you seen? And, and how did you like the output? All of this would be really cool to hear.

I also don’t know when this happened, but I checked, and inside of LinkedIn’s newsfeed, it used to have an ad every five slots. But right now it’s every four. I’m really unhappy about it. As an advertiser, you’d think I’d be excited, since this would mean that we would have more impressions on our ads. But I’ll also tell you, as an advertiser, I really want people to fall in love with the platform and want to come use it a lot and not be annoyed so that they’ll come back and keep seeing our ads over and over. So, anyone paying attention and know when this was rolled out, the move from an ad every five to every four, I’d be really interested in hearing. It’s also possible that I’m part of a limited test right now, but either way, I’m really not happy with it. I, I remember when Facebook moved from an ad every five to an ad every four. And I personally knew several people who were like, I’m just not going to spend any time on Facebook anymore. This is ridiculous. It’s more ads than it is content. It feels like. And I’d love to hear your opinions on this as well.

I also want to feature a review that was left on Apple podcasts. The heading said, &quot;One of the best podcasts out there. It reads, AJ’s podcast is easily one of the best podcasts I listen to. Every episode is packed with actionable takeaways that you can implement right away. His episodes have the perfect amount of information that leave you feeling excited to test new strategies, hacks, or whatever he’s talking about. But not too much that you feel overwhelmed. His podcast is for beginners and experts, and everyone in between. You can tell A. J. puts a lot of time and thought into every episode, and all the content he creates. He’s always happy and willing to share new things that have worked for him, and genuinely excited to hear others wins and strategies. If you haven’t already, go subscribe to his podcast because you’re not gonna want to miss a single episode, and then go sign up for his fanatics community. Thanks, AJ.&quot; Alright, so this review was left by AHY with a bunch of numbers. Now, AHY, I don’t know who you are from your username, but a huge thank you for leaving such a kind review. I’m so glad that you’ve gotten so much out of the show. It warms my heart when I get to hear that the efforts that I’m making and that all of us here are putting into the show are making a difference in your performance and in your life.

I was especially happy to hear that there’s just enough information without overloading you. I do occasionally feel like I overload you guys as an audience. But there’s so much that I want to share with you that I have up here in this head that I just need to get out. So, I apologize if there’s anyone out there who maybe you feel like you get drowned with information. But, of course, we’re trying really hard to make sure that it’s a little bit more bite sized.

Alright, so anyone, if you have a question, a review, or feedback for the show, message me on LinkedIn or email us at Podcast@B2Linked.com. You can attach a link to a voice recording from you and I’ll play them right here on the show. I’m happy to keep you anonymous or share your details as well and shout you out. So record yourself asking a question, commenting on something from a past episode, and I’ll aim to include you right here in the show. And of course, I want to shout you out if you leave reviews for the show as well. So Apple Podcasts, by far the best place to do that.

Alright, with all that out of the way, let’s hit it. This week, we’re talking about running AAB tests. And although I do occasionally stutter, you didn’t mishear me. I’ve been running AAB tests on LinkedIn for 10 plus years now. They’ve been extremely helpful for me in interpreting the results of my AB tests.

What is an AAB test? It’s pretty dang close to what it sounds like. Instead of an AB test, where you run two ads against each other with a slight variation, with an AAB test, you run three ads against each Where two of them are A’s, they’re identical, and your B is a single, unique ad. Now, I know at this point you’re asking, AJ, why in the world would you do that? Why would you go and launch an extra ad for no reason? Well, I assure you, there is a good reason. And that’s because when you run an A B test on LinkedIn, there are all kinds of challenges that get in your way. For instance, LinkedIn doesn’t show your ads evenly, no matter what you do when you launch the ads.

LinkedIn defaults to a setting called optimize your ads for performance, which means that whichever of your two ads has a better engagement rate, LinkedIn is going to favor it with more impressions and a lot more traffic, most likely

Now there actually is an option to rotate your ads evenly. But in my opinion, it’s a total misnomer. What it actually does is it enters both ads into the auction evenly. But the ad that has the lower engagement rate will naturally be assigned a lower relevancy score. Which means when both are entered into the auction evenly, the one with the higher relevancy score, which would normally be entered into the auction most, is rotated evenly with the lower performing ad. But because the lower performing ad has a lower relevancy score, it loses more of those auctions. So you end up getting fewer impressions and less traffic. But, when it does win the auction, it does so at a higher cost, because it has a lower relevancy score. So, you’ll find that when you switch to this option of rotate your ads evenly, your costs will jump up, and your impressions and traffic will drop. That’s why I call this option the rotate evenly option, the charge me more and show me less button.

Now, if that totally went over your head, I totally understand. You’ll want to go listen to episode 108, all about the auction. That’s where we go really deep into how relevancy score affects your ads being shown and clicked and all of that. You may also want to check out episode 36, that’s a deep dive into AB testing.

Okay, so at this point you might be stopping me and saying, AJ, didn’t you know that LinkedIn has an AB testing tool that you can use? And yes, I’m very well aware of it. You find this by going to test over in the left hand navigation. And you create a test and one of the options is AB test. That being said, I don’t recommend this because what it does when it creates a new test is it creates two brand new campaigns and puts a new ad into each one. And so much of the benefit of running evergreen campaigns on LinkedIn is that We are investing into history in campaigns, and having that history in our campaigns means that when we launch something new in it, we get the benefit of that history. That’s preferred pricing and preferred delivery, because when we launch new ads into them, LinkedIn doesn’t have to wonder how credible the advertiser is because it can look in history and see we’ve spent thousands, tens of thousands, hundreds of thousands of dollars on this campaign. So you end up getting really the benefit of the doubt from the LinkedIn auction. And so when you use this AB testing tool, You’re creating new campaigns that you just have to throw away afterwards.

I’ll also tell you that LinkedIn’s auction performance is sometimes hard to understand and interpret, and it’s even flawed occasionally. But how could I even tell that that’s the case? Well, you can tell by actually doing what we’re talking about today, running an AAB test. This allows us to run an AB test, and at the same time, get a temperature check for how much confidence you should actually place in the outcome of this AB test.

Now this works because if your two A’s in the test that are identical have vastly different performance, you can tell there’s some irregularity in the test, and you may not be able to trust it. If your As, though, are really similar in performance, you can have the confidence that your AB test results are likely sound, and you can trust them.

Alright, so how do you evaluate these AAB tests? Well, most of the time with an AB test, I’m evaluating the creative and the copy of an ad. Which imagery is the most eye catching? Well, that’s going to show up in your click through rates, because the more of a thumb stopper your image is, the more people are going to read your ad copy and see they actually do want to click.

And then what about your ad copy? You have a motivation or hook in that copy, and that’s trying to grab attention, and it’s giving them a strong call to action, and tells them why they should be paying attention. So, this also affects your click through rates. Now, it can influence your conversion rates as well, but that’s usually not the primary metric that I’m evaluating these AB tests on.

So since click through rate is the main metric that we’re going to be watching in our AB tests, what you do is you start watching for large differences in your click through rates between the elements that you’re testing. And after you’ve spent enough on the test to have enough data and to make the results significant, that’s when you get to compare between the two variants and make a judgment call about which of these is actually a better ad for your audience. Now, if you want to learn more about statistical significance, go Check out episode 57, where I had Chris Daly on the show, and we talked all about statistical significance, a word I obviously can’t say.

Now, from my experience on LinkedIn Ads, I’ve found that very often in an AB test, my click through rates become significant after about $1,000 in spend across a test. But of course this is just a rule of thumb, because this is going to totally depend on the performance of those ads. For instance, last week I was evaluating an A B test for a client, and we had several thousand dollars spent across an AB test, and the difference between their click through rates was next to nothing. One had a 0.78 percent click through rate, and the other one had a 0.8%, so just 0.02 difference. So even though we’d spent, like, a lot of money, the results were still not significant.

On the other hand, though, if your ads get click through rates that are very different, like, let’s say one has a 0.6 percent and the other has a 0.9 percent CTR, you might achieve statistical significance after just a few hundred dollars spent.

Now, occasionally I get asked this question, well, if I launch an AAB test, won’t LinkedIn realize that the two A’s are identical and treat them the same? This is a really smart question, and the answer might surprise you. No, if you create the ads as direct sponsored content, which means unique ads that you’ve created within Campaign Manager, they’re going to have different creative IDs on the backend, and so LinkedIn can’t tell the difference.

Even though they’re effectively the same. They have the same image, the same intro text, the same headline. LinkedIn’s not going to notice that they’re different. And honestly, LinkedIn doesn’t even care if they’re identical. You’re not doing anything sneaky here. So if you sponsor existing ads, rather than creating them from scratch in Campaign Manager, you’re not doing anything sneaky here. They actually will have the same creative ID on the backend and LinkedIn will assume they are the same ad because really they are. So it’s important that when you run AAB tests, you’re creating each of these ads from scratch in the same campaign, all within campaign manager.

Now, I wanted to talk about AAB tests because this, just this month, I actually ran an AAB test where the A’s were so far apart, I actually threw the results of the AAB test clean out entirely. And I ended up starting from scratch a brand new test. What happened is the click through rates of the A’s were 0.3 percent and 0.78%.

I mean, we’re talking like worlds of differences here. And then you compare that with the B ad that had a 0.54 percent click through rate, so right in between.

And I knew I couldn’t trust this test because the same exact ad having a 0.3 percent and a 0.78 percent click through rate over a couple thousand dollars spent makes absolutely no sense. I can’t come up with a reason that would explain why the same ad would have such varying difference.

So I’d love to hear if you’ve launched AAB tests in the past, what your experience was. And I’d also love to hear the results of your AAB tests after you hear this episode, and then go launch one of your own. Message me on LinkedIn, or email us at Podcast@B2linked.Com, and I’d love to hear all about your experience.

Okay, in the episode show notes, you’ll see links to each of these resources, but I have a link to the announcement of LinkedIn Accelerator, the AI based creation of your campaign and ads. As well as the announcement about Thought Leader Ads being able to promote posts of a non employee. That was super exciting. We also have a deep dive into the LinkedIn Ads auction, all about relevancy score. That’s episode 108, if you haven’t heard that. We also have the episode all about AB testing that does a deep dive. That’s episode 36. I also mentioned episode 57, where we had Chris Daly come talk about statistical significance. That one is an absolute must listen all about the LinkedIn Ads testing methodology.

Now, if you’re not already a member of the LinkedIn ads fanatics community, what are you waiting for? Go to fanatics.b2linked.com and sign up. For a very low monthly fee, you’ll get access to all of the top minds in LinkedIn Ads, all sharing what’s working for them, bouncing ideas off each other. It’s a fantastic place to learn more and buoy up all of the performance that you’re seeing.

As a member of the LinkedIn Ads fanatics, you’ll also have free access to all four of our courses that take you from absolute beginner to total LinkedIn ads expert and pro, maybe even ninja.

So again, that’s fanatics.b2linked.com. Now, if this is your first time listening, welcome, we’re super excited to have you here. Make sure to hit that subscribe button so you hear us again next week, but if this is not your first time listening, if you’re a loyal listener, the very best thing that you can do is go and leave us a review on Apple Podcasts. That is by far the very best way you can say thank you for the hours and hours every week that it takes to put this show together and give you the absolute best of all of the LinkedIn ads strategies that we’ve researched and, and ideated over the years.

Now, do you have questions, suggestions, or corrections for the show? Reach out to us at Podcast@B2linked.com. And with that being said, we’ll see you back here next week. I’m cheering you on in your LinkedIn Ads initiatives.

‍